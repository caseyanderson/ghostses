{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ghostses:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\" setup the object \"\"\"\n",
    "        self.filename = filename\n",
    "        self.corpus = None # plaintext of the corpus\n",
    "        self.tokens = None # tokenized corpus\n",
    "        self.pos = None # tokenized corpus with parts of speech [token, pos]\n",
    "        self.colorized = {} # dict to store the colorized parts of speech\n",
    "\n",
    "\n",
    "    def readCorpus(self):\n",
    "        \"\"\" read the corpus into object \"\"\"\n",
    "        f = open(str(self.filename), 'r')\n",
    "        self.corpus = f.read()\n",
    "\n",
    "\n",
    "    def getTokens(self, whitespace = False):\n",
    "        \"\"\" tokenize corpus \"\"\"\n",
    "        if whitespace == False:\n",
    "            self.tokens = nltk.word_tokenize(str(self.corpus))\n",
    "        elif whitespace == True:\n",
    "            temp = [[word_tokenize(w), ' '] for w in self.corpus.split()]\n",
    "            self.tokens = list(itertools.chain(*list(itertools.chain(*temp))))\n",
    "\n",
    "\n",
    "    def getPOS(self):\n",
    "        \"\"\" run parts of speech analysis on tokens\n",
    "            converts and stores output as 2d list\n",
    "            [ token, pos ] \"\"\"\n",
    "        pos = nltk.pos_tag(self.tokens)\n",
    "        self.pos = list(map(list, pos))\n",
    "\n",
    "\n",
    "    def colorizer(self, spch, dct):\n",
    "        \"\"\" get NLTK tags for part of speech\n",
    "            find all words that match part of speech and wrap matches in color <span>\n",
    "            all other words are wrapped in whitespace <span>\n",
    "            output to dict at self.colorized\n",
    "        \"\"\"\n",
    "        labels = dct\n",
    "        speech = spch\n",
    "        size = len(self.pos)\n",
    "        colorized = [None] * size\n",
    "        for x in labels[speech]:\n",
    "            # print(\"looking for \" + x)\n",
    "            step = 0\n",
    "            for y in self.pos:\n",
    "                if y[1] == x:\n",
    "                    print(\"found \" + x + \" at \" + str(step) + \" : \" + y[0])\n",
    "                    colorizedToken = \"<span class='\" + str(speech) + \"'>\" + str(y[0]) + \"</span>\"\n",
    "                    colorized[step] = colorizedToken\n",
    "                step+=1\n",
    "        step = 0\n",
    "        for z in colorized:\n",
    "            if z == None:\n",
    "                word = self.pos[step][0]\n",
    "                print(word)\n",
    "                whitespacedToken = \"<span class='whitespace'>\" + str(word) + \"</span>\"\n",
    "                colorized[step] = whitespacedToken\n",
    "            step+=1\n",
    "        self.colorized[str(speech)] = colorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Ghostses('corpus_mini.txt')\n",
    "test.readCorpus()\n",
    "test.getTokens()\n",
    "test.getPOS()\n",
    "\n",
    "#refactor the next two lines\n",
    "dctnry={}\n",
    "keys = ['noun', 'adj', 'vrb', 'advrb','symb', 'background']\n",
    "\n",
    "dctnry['noun'] = [ 'NN', 'NNP', 'NNPS', 'NNS']\n",
    "dctnry['adj'] = ['JJ', 'JJR', 'JJS']\n",
    "dctnry['vrb'] = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "dctnry['advrb'] = ['RB', 'RBR', 'RBS']\n",
    "dctnry['background'] = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'LS', 'MD', 'PDT', 'POS', 'PRP', 'PRP$', 'RP', 'SYM', 'TO', 'UH', 'WDT', 'WP', 'WP$', 'WRP']\n",
    "dctnry['symb'] = ['$', \"''\", '(', ')', ',', '--', '.', ':', \"''\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'Janine',\n",
       " 'who',\n",
       " 'referred',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'surgeon',\n",
       " 'Anthony',\n",
       " 'Batty',\n",
       " 'Shaw',\n",
       " ',',\n",
       " 'whom',\n",
       " 'she',\n",
       " 'knew',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Oxford',\n",
       " 'Society',\n",
       " ',',\n",
       " 'when',\n",
       " 'after',\n",
       " 'my',\n",
       " 'discharge',\n",
       " 'from',\n",
       " 'hospital',\n",
       " 'I',\n",
       " 'began',\n",
       " 'my',\n",
       " 'enquiries',\n",
       " 'about',\n",
       " 'Thomas',\n",
       " 'Browne',\n",
       " ',',\n",
       " 'who',\n",
       " 'had',\n",
       " 'practised',\n",
       " 'as',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'in',\n",
       " 'Norwich',\n",
       " 'in',\n",
       " 'the',\n",
       " 'seventeenth',\n",
       " 'century',\n",
       " 'and',\n",
       " 'had',\n",
       " 'left',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'writings',\n",
       " 'that',\n",
       " 'defy',\n",
       " 'all',\n",
       " 'comparison',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It', 'PRP'],\n",
       " ['was', 'VBD'],\n",
       " ['Janine', 'NNP'],\n",
       " ['who', 'WP'],\n",
       " ['referred', 'VBD'],\n",
       " ['me', 'PRP'],\n",
       " ['to', 'TO'],\n",
       " ['the', 'DT'],\n",
       " ['surgeon', 'NN'],\n",
       " ['Anthony', 'NNP'],\n",
       " ['Batty', 'NNP'],\n",
       " ['Shaw', 'NNP'],\n",
       " [',', ','],\n",
       " ['whom', 'WP'],\n",
       " ['she', 'PRP'],\n",
       " ['knew', 'VBD'],\n",
       " ['from', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['Oxford', 'NNP'],\n",
       " ['Society', 'NNP'],\n",
       " [',', ','],\n",
       " ['when', 'WRB'],\n",
       " ['after', 'IN'],\n",
       " ['my', 'PRP$'],\n",
       " ['discharge', 'NN'],\n",
       " ['from', 'IN'],\n",
       " ['hospital', 'NN'],\n",
       " ['I', 'PRP'],\n",
       " ['began', 'VBD'],\n",
       " ['my', 'PRP$'],\n",
       " ['enquiries', 'NNS'],\n",
       " ['about', 'IN'],\n",
       " ['Thomas', 'NNP'],\n",
       " ['Browne', 'NNP'],\n",
       " [',', ','],\n",
       " ['who', 'WP'],\n",
       " ['had', 'VBD'],\n",
       " ['practised', 'VBN'],\n",
       " ['as', 'IN'],\n",
       " ['a', 'DT'],\n",
       " ['doctor', 'NN'],\n",
       " ['in', 'IN'],\n",
       " ['Norwich', 'NNP'],\n",
       " ['in', 'IN'],\n",
       " ['the', 'DT'],\n",
       " ['seventeenth', 'JJ'],\n",
       " ['century', 'NN'],\n",
       " ['and', 'CC'],\n",
       " ['had', 'VBD'],\n",
       " ['left', 'VBN'],\n",
       " ['a', 'DT'],\n",
       " ['number', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['writings', 'NNS'],\n",
       " ['that', 'WDT'],\n",
       " ['defy', 'VBP'],\n",
       " ['all', 'DT'],\n",
       " ['comparison', 'NN'],\n",
       " ['.', '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5dbf33c7972c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1d0e44c3b61a>\u001b[0m in \u001b[0;36mgetTokens\u001b[0;34m(self, whitespace)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mwhitespace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1d0e44c3b61a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mwhitespace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "test.getTokens(whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was Janine who referred me to the surgeon Anthony Batty Shaw, whom she knew from the Oxford Society, when after my discharge from hospital I began my enquiries about Thomas Browne, who had practised as a doctor in Norwich in the seventeenth century and had left a number of writings that defy all comparison.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-35c3ad771d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'Janine',\n",
       " 'who',\n",
       " 'referred',\n",
       " 'me',\n",
       " 'to',\n",
       " 'the',\n",
       " 'surgeon',\n",
       " 'Anthony',\n",
       " 'Batty',\n",
       " 'Shaw,',\n",
       " 'whom',\n",
       " 'she',\n",
       " 'knew',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Oxford',\n",
       " 'Society,',\n",
       " 'when',\n",
       " 'after',\n",
       " 'my',\n",
       " 'discharge',\n",
       " 'from',\n",
       " 'hospital',\n",
       " 'I',\n",
       " 'began',\n",
       " 'my',\n",
       " 'enquiries',\n",
       " 'about',\n",
       " 'Thomas',\n",
       " 'Browne,',\n",
       " 'who',\n",
       " 'had',\n",
       " 'practised',\n",
       " 'as',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'in',\n",
       " 'Norwich',\n",
       " 'in',\n",
       " 'the',\n",
       " 'seventeenth',\n",
       " 'century',\n",
       " 'and',\n",
       " 'had',\n",
       " 'left',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'writings',\n",
       " 'that',\n",
       " 'defy',\n",
       " 'all',\n",
       " 'comparison.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was Janine who referred me to the surgeon Anthony Batty Shaw, whom she knew from the Oxford Society, when after my discharge from hospital I began my enquiries about Thomas Browne, who had practised as a doctor in Norwich in the seventeenth century and had left a number of writings that defy all comparison.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ghostses:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\" setup the object \"\"\"\n",
    "        self.filename = filename\n",
    "        self.corpus = None # plaintext of the corpus\n",
    "        self.tokens = None # tokenized corpus\n",
    "        self.pos = None # tokenized corpus with parts of speech [token, pos]\n",
    "        self.colorized = {} # dict to store the colorized parts of speech\n",
    "\n",
    "\n",
    "    def readCorpus(self):\n",
    "        \"\"\" read the corpus into object \"\"\"\n",
    "        f = open(str(self.filename), 'r')\n",
    "        self.corpus = f.read()\n",
    "\n",
    "\n",
    "    def getTokens(self, whitespace = False):\n",
    "        \"\"\" tokenize corpus \"\"\"\n",
    "        if whitespace == False:\n",
    "            self.tokens = nltk.word_tokenize(str(self.corpus))\n",
    "        elif whitespace == True:\n",
    "\n",
    "            temp = [[nltk.word_tokenize(w), ' '] for w in self.corpus.split()]\n",
    "            self.tokens = list(itertools.chain(*list(itertools.chain(*temp))))\n",
    "\n",
    "\n",
    "    def getPOS(self):\n",
    "        \"\"\" run parts of speech analysis on tokens\n",
    "            converts and stores output as 2d list\n",
    "            [ token, pos ] \"\"\"\n",
    "        pos = nltk.pos_tag(self.tokens)\n",
    "        self.pos = list(map(list, pos))\n",
    "\n",
    "\n",
    "    def colorizer(self, spch, dct):\n",
    "        \"\"\" get NLTK tags for part of speech\n",
    "            find all words that match part of speech and wrap matches in color <span>\n",
    "            all other words are wrapped in whitespace <span>\n",
    "            output to dict at self.colorized\n",
    "        \"\"\"\n",
    "        labels = dct\n",
    "        speech = spch\n",
    "        size = len(self.pos)\n",
    "        colorized = [None] * size\n",
    "        for x in labels[speech]:\n",
    "            # print(\"looking for \" + x)\n",
    "            step = 0\n",
    "            for y in self.pos:\n",
    "                if y[1] == x:\n",
    "                    print(\"found \" + x + \" at \" + str(step) + \" : \" + y[0])\n",
    "                    colorizedToken = \"<span class='\" + str(speech) + \"'>\" + str(y[0]) + \"</span>\"\n",
    "                    colorized[step] = colorizedToken\n",
    "                step+=1\n",
    "        step = 0\n",
    "        for z in colorized:\n",
    "            if z == None:\n",
    "                word = self.pos[step][0]\n",
    "                print(word)\n",
    "                whitespacedToken = \"<span class='whitespace'>\" + str(word) + \"</span>\"\n",
    "                colorized[step] = whitespacedToken\n",
    "            step+=1\n",
    "        self.colorized[str(speech)] = colorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ghostses:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\" setup the object \"\"\"\n",
    "        self.filename = filename\n",
    "        self.corpus = None # plaintext of the corpus\n",
    "        self.tokens = None # tokenized corpus\n",
    "        self.pos = None # tokenized corpus with parts of speech [token, pos]\n",
    "        self.colorized = {} # dict to store the colorized parts of speech\n",
    "\n",
    "\n",
    "    def readCorpus(self):\n",
    "        \"\"\" read the corpus into object \"\"\"\n",
    "        f = open(str(self.filename), 'r')\n",
    "        self.corpus = f.read()\n",
    "\n",
    "\n",
    "    def getTokens(self, whitespace = False):\n",
    "        \"\"\" tokenize corpus \"\"\"\n",
    "        if whitespace == False:\n",
    "            self.tokens = nltk.word_tokenize(str(self.corpus))\n",
    "        elif whitespace == True:\n",
    "\n",
    "            temp = [[nltk.word_tokenize(w), ' '] for w in self.corpus.split()]\n",
    "            self.tokens = list(itertools.chain(*list(itertools.chain(*temp))))\n",
    "\n",
    "\n",
    "    def getPOS(self):\n",
    "        \"\"\" run parts of speech analysis on tokens\n",
    "            converts and stores output as 2d list\n",
    "            [ token, pos ] \"\"\"\n",
    "        pos = nltk.pos_tag(self.tokens)\n",
    "        self.pos = list(map(list, pos))\n",
    "\n",
    "\n",
    "    def colorizer(self, spch, dct):\n",
    "        \"\"\" get NLTK tags for part of speech\n",
    "            find all words that match part of speech and wrap matches in color <span>\n",
    "            all other words are wrapped in whitespace <span>\n",
    "            output to dict at self.colorized\n",
    "        \"\"\"\n",
    "        labels = dct\n",
    "        speech = spch\n",
    "        size = len(self.pos)\n",
    "        colorized = [None] * size\n",
    "        for x in labels[speech]:\n",
    "            # print(\"looking for \" + x)\n",
    "            step = 0\n",
    "            for y in self.pos:\n",
    "                if y[1] == x:\n",
    "                    print(\"found \" + x + \" at \" + str(step) + \" : \" + y[0])\n",
    "                    colorizedToken = \"<span class='\" + str(speech) + \"'>\" + str(y[0]) + \"</span>\"\n",
    "                    colorized[step] = colorizedToken\n",
    "                step+=1\n",
    "        step = 0\n",
    "        for z in colorized:\n",
    "            if z == None:\n",
    "                word = self.pos[step][0]\n",
    "                print(word)\n",
    "                whitespacedToken = \"<span class='whitespace'>\" + str(word) + \"</span>\"\n",
    "                colorized[step] = whitespacedToken\n",
    "            step+=1\n",
    "        self.colorized[str(speech)] = colorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Ghostses('corpus_mini.txt')\n",
    "test.readCorpus()\n",
    "# test.getTokens()\n",
    "# test.getPOS()\n",
    "\n",
    "#refactor the next two lines\n",
    "dctnry={}\n",
    "keys = ['noun', 'adj', 'vrb', 'advrb','symb', 'background']\n",
    "\n",
    "dctnry['noun'] = [ 'NN', 'NNP', 'NNPS', 'NNS']\n",
    "dctnry['adj'] = ['JJ', 'JJR', 'JJS']\n",
    "dctnry['vrb'] = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "dctnry['advrb'] = ['RB', 'RBR', 'RBS']\n",
    "dctnry['background'] = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'LS', 'MD', 'PDT', 'POS', 'PRP', 'PRP$', 'RP', 'SYM', 'TO', 'UH', 'WDT', 'WP', 'WP$', 'WRP']\n",
    "dctnry['symb'] = ['$', \"''\", '(', ')', ',', '--', '.', ':', \"''\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.getTokens(whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " ' ',\n",
       " 'was',\n",
       " ' ',\n",
       " 'Janine',\n",
       " ' ',\n",
       " 'who',\n",
       " ' ',\n",
       " 'referred',\n",
       " ' ',\n",
       " 'me',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'surgeon',\n",
       " ' ',\n",
       " 'Anthony',\n",
       " ' ',\n",
       " 'Batty',\n",
       " ' ',\n",
       " 'Shaw',\n",
       " ',',\n",
       " ' ',\n",
       " 'whom',\n",
       " ' ',\n",
       " 'she',\n",
       " ' ',\n",
       " 'knew',\n",
       " ' ',\n",
       " 'from',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'Oxford',\n",
       " ' ',\n",
       " 'Society',\n",
       " ',',\n",
       " ' ',\n",
       " 'when',\n",
       " ' ',\n",
       " 'after',\n",
       " ' ',\n",
       " 'my',\n",
       " ' ',\n",
       " 'discharge',\n",
       " ' ',\n",
       " 'from',\n",
       " ' ',\n",
       " 'hospital',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'began',\n",
       " ' ',\n",
       " 'my',\n",
       " ' ',\n",
       " 'enquiries',\n",
       " ' ',\n",
       " 'about',\n",
       " ' ',\n",
       " 'Thomas',\n",
       " ' ',\n",
       " 'Browne',\n",
       " ',',\n",
       " ' ',\n",
       " 'who',\n",
       " ' ',\n",
       " 'had',\n",
       " ' ',\n",
       " 'practised',\n",
       " ' ',\n",
       " 'as',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'doctor',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'Norwich',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'seventeenth',\n",
       " ' ',\n",
       " 'century',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'had',\n",
       " ' ',\n",
       " 'left',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'number',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'writings',\n",
       " ' ',\n",
       " 'that',\n",
       " ' ',\n",
       " 'defy',\n",
       " ' ',\n",
       " 'all',\n",
       " ' ',\n",
       " 'comparison',\n",
       " '.',\n",
       " ' ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
